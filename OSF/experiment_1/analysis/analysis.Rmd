---
title: "Physical Pragmatics (Experiment 1)"
output:
  pdf_document: default
  html_document:
    code_folding: hide
    df_print: paged
---
  
```{r setup, echo=FALSE}
# Set the working directory.
knitr::opts_knit$set(root.dir=normalizePath(".."))

# Turn off compile messages and warnings.
knitr::opts_chunk$set(message=FALSE, warning=FALSE)

# Set the seed value.
seed = 0

# Set up the path to each partition of the participant data.
data_0_path = "data/data_0"
data_1_path = "data/data_1"
data_2_path = "data/data_2"
```

```{r libraries, echo=FALSE}
# Import R libraries.
library(boot)
library(jsonlite)
library(tidyverse)
```

# Preprocessing

```{r preprocessing}
# Read in each partition of the participant data.
# NOTE: VEOnwX4 had a duplicate entry. Deleted the latter one.
data_6 = tibble()
for (data_path in c(data_0_path, data_1_path, data_2_path)) {
  # Read in the participant data for this partition.
  data_0 = read_csv(file.path(data_path, "raw_data.csv"), quote="~")
  
  # Read in the MTurk results file for this partition.
  mturk_results = read_csv(file.path(data_path, "mturk_results.csv")) %>%
    mutate(Answer.surveycode=str_sub(Answer.surveycode, 3)) %>%
    rename(unique_id=Answer.surveycode)
  
  # Raise an error if there is a mismatch between the data and MTurk results 
  # file.
  mismatch = length(setdiff(data_0$unique_id, mturk_results$unique_id)) != 0
  duplicates = length(unique(data_0$unique_id)) != length(data_0$unique_id)
  if (mismatch | duplicates) {
    stop(paste("There's a mismatch/duplicate between the data and the MTurk ", 
               "results file.", sep=""))
  }
  
  # Convert the JSON string into JSON.
  data_1 = lapply(data_0$results, fromJSON)
  
  # Extract the trial information for each participant and stack them.
  data_3 = tibble()
  for (p in 1:length(data_1)) {
    # Trim the map and add the participant ID back in.
    data_2 = data_1[p][[1]]$trials %>%
      as.data.frame() %>%
      gather(type, num, trial_num, exclusion_num) %>%
      na.omit() %>%
      mutate(type=gsub("num", "", type)) %>%
      mutate(target=as.numeric(target), trial=paste(type, num, sep=""),
             object=data_1[p][[1]]$setup$object,
             unique_id=data_0$unique_id[p],
             age=as.numeric(data_1[p][[1]]$subject_information$age)) %>%
      select(-type, -num)
    
    # Stack the trial information for the current participant.
    data_3 = rbind(data_3, data_2)
  }
  
  # Exclude participants that said they couldn't walk through one of the doors.
  data_5 = tibble()
  exclusions = tibble()
  for (participant in unique(data_3$unique_id)) {
    data_4 = data_3 %>%
      filter(unique_id==participant)
    
    if (filter(data_4, trial=="exclusion_2")[,"target"]==0) {
      data_5 = rbind(data_5, data_4)
    }
    else if (filter(data_4, trial=="exclusion_2")[,"target"]==1) {
      exclusions = rbind(exclusions, data_4)    
    }
  }
  
  # Write the preprocessed data.
  write_csv(data_5, file.path(data_path, "data.csv"))
  
  # Append the preprocessed data.
  data_6 = rbind(data_6, data_5)
}
```

# Compute Door Endorsement

## _a priori_ Labels

First, we'll analyze the participant endorsement ($N$=`r nrow(filter(data_6, trial=="trial_1"))`; $M$=`r round(mean(filter(data_6, trial=="trial_1")$age, na.rm=TRUE), 2)` years, $SD$=`r round(sd(filter(data_6, trial=="trial_1")$age, na.rm=TRUE), 2)` years) for the low-cost door (i.e., if they believed the low-cost door to be more likely to have a communicative meaning) using _a priori_ labels of door difficulty.

```{r compute_endorsement_a_priori_labels, fig.align="center"}
# Read in the preprocessed data.
data_6 = read_csv(file.path(data_0_path, "data.csv")) %>%
  rbind(read_csv(file.path(data_1_path, "data.csv"))) %>%
  rbind(read_csv(file.path(data_2_path, "data.csv")))

# Filter the trial of interest.
data_7 = data_6 %>%
  filter(trial=="trial_1")

# Set up the bootstrap functions.
compute_mean = function(data, indices) {
  return(mean(data[indices]))
}

compute_bootstrap = function(data) {
  simulations = boot(data=data$target,
                     statistic=compute_mean,
                     R=10000)
  
  return(boot.ci(simulations, type="perc")$perc)
}

# Set the seed and compute the 95% bootstrapped CI.
set.seed(seed)
bootstrap_data = compute_bootstrap(data_7)
lower_ci = bootstrap_data[4] * 100
upper_ci = bootstrap_data[5] * 100

# Convert individual judgments to aggregate percentages.
data_8 = data_7 %>%
  group_by(target) %>%
  summarize(percentage=n()/nrow(data_7)*100) %>%
  mutate(target=as.character(target))

# Plot the data (using our labels of door difficulty).
plot_0 = data_8 %>%
  ggplot(aes(x="", y=percentage, fill=target)) + 
  geom_bar(stat="identity", width=0.5) +
  geom_errorbar(aes(ymin=lower_ci, ymax=upper_ci), width=0.15) +
  geom_hline(yintercept=50, linetype="dashed", color="black") +
  theme_classic() +
  theme(aspect.ratio=1.0,
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        legend.title=element_text(hjust=0.5)) +
  ylab("Endorsement (%)") +
  scale_fill_discrete(name="Door",
                      limits=c("0", "1"),
                      labels=c("No-Cost", "Low-Cost"))
plot_0
```

```{r compute_statistics_a_priori_labels}
# Compute a binomial test with the alternative hypothesis that the participant 
# endorsement of the low-cost door is above chance.
# NOTE: Computing a two-sided binomial test for "conservativeness".
binom.test(x=sum(data_7$target), n=length(data_7$target), p=0.5,
           alternative="two.sided")
```

## Participant Labels

Now, we'll analyze the participant endorsement ($N$=`r nrow(filter(data_6, trial=="trial_1"))`; $M$=`r round(mean(filter(data_6, trial=="trial_1")$age, na.rm=TRUE), 2)` years, $SD$=`r round(sd(filter(data_6, trial=="trial_1")$age, na.rm=TRUE), 2)` years) for the low-cost door (i.e., if they believed the low-cost door to be more likely to have a communicative meaning) using participant labels of door difficulty.

```{r compute_endorsement_participant_labels, fig.align="center"}
# Condition the target data based on which door the participants saw as more
# challenging to walk through (instead of using our cost labels).
data_9 = data_6 %>%
  filter(trial %in% c("trial_1", "exclusion_1")) %>%
  spread(trial, target) %>%
  mutate(target=ifelse(trial_1==1 & exclusion_1==1, 1,
                       ifelse(trial_1==0 & exclusion_1==0, 1, 0)))

# Set the seed and compute the 95% bootstrapped CI.
set.seed(seed)
bootstrap_data = compute_bootstrap(data_9)
lower_ci = bootstrap_data[4] * 100
upper_ci = bootstrap_data[5] * 100

# Convert individual judgments to aggregate percentages.
data_10 = data_9 %>%
  group_by(target) %>%
  summarize(percentage=n()/nrow(data_9)*100) %>%
  mutate(target=as.character(target))

# Plot the data (using participant labels of door difficulty).
plot_1 = data_10 %>%
  ggplot(aes(x="", y=percentage, fill=target)) + 
  geom_bar(stat="identity", width=0.5) +
  geom_errorbar(aes(ymin=lower_ci, ymax=upper_ci), width=0.15) +
  geom_hline(yintercept=50, linetype="dashed", color="black") +
  theme_classic() +
  theme(aspect.ratio=1.0,
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        legend.title=element_text(hjust=0.5)) +
  ylab("Endorsement (%)") +
  scale_fill_discrete(name="Door",
                      limits=c("0", "1"),
                      labels=c("No-Cost", "Low-Cost"))
plot_1
```

```{r compute_statistics_participant_labels}
# Compute a binomial test on the alternative hypothesis that the participant 
# endorsement for the no-cost/low-cost door is above chance.
# NOTE: Computing a two-sided binomial test for "conservativeness".
binom.test(x=sum(data_9$target), n=length(data_9$target), p=0.5,
           alternative="two.sided")
```
