---
title: "Physical Pragmatics (model)"
output:
  pdf_document: default
  html_document:
    code_folding: hide
    df_print: paged
---
  
```{r setup, echo=FALSE}
# Set the working directory.
knitr::opts_knit$set(root.dir=normalizePath("../.."))

# Turn off compile messages and warnings.
knitr::opts_chunk$set(message=FALSE, warning=FALSE)

# Set a custom color palette for plotting.
color_palette = c("#999999", "#E69F00", "#56B4E9", "#009E73",
                  "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# Set up the path to the model data.
data_path = "data/model"
```

```{r libraries, echo=FALSE}
# Import R libraries.
library(tidyverse)
```

# Model Analysis

Here we compute model predictions for our two kinds of agents: enforcers and deciders.

```{r enforcer_model, fig.align="center"}
# Read in the model predictions for the (standard) enforcer model.
enforcer_0 = read_csv(file.path(data_path,
                                "standard_enforcer_1.0_1.0_0.0_0.0.csv"))

# Transform the decider rewards and enforcer actions from 2-d to 1-d.
enforcer_1 = enforcer_0 %>%
  mutate(decider_reward=decider_reward_0-decider_reward_1,
         enforcer_action=enforcer_action_0-enforcer_action_1)

# Filter the relevant model predictions for the (standard) enforcer model.
enforcer_2 = enforcer_1 %>%
  filter(rationality==0.1, enforcer_reward_0==0, enforcer_reward_1==9,
         ToM %in% c(0.0, 1.0), method=="confidence", cooperation==10,
         decider_reward %in% c(-3, -2, -1, 0, 1, 2, 3)) %>%
  select(ToM, decider_reward, enforcer_action)

# Plot the model predictions for the (standard) enforcer model.
plot_0 = enforcer_2 %>%
  mutate(ToM=factor(ToM)) %>%
  ggplot(aes(x=decider_reward, y=enforcer_action, group=ToM, color=ToM)) +
  geom_line(size=2, alpha=0.75) +
  theme_classic() +
  theme(aspect.ratio=1.0) +
  scale_x_discrete(name="Decider Reward",
                   limits=c(-3, -2, -1, 0, 1, 2, 3),
                   labels=c(-3, -2, -1, 0, 1, 2, 3)) +
  ylab("Objects Stacked") +
  scale_color_manual(name="Decider Type:",
                     limits=c("0", "1"),
                     labels=c("No ToM", "ToM"),
                     values=c(color_palette[3], color_palette[2]))
plot_0
```

```{r decider_model, fig.align="center"}
# Read in the model predictions for the (standard) decider model.
decider_0 = read_csv(file.path(data_path,
                               "standard_decider_1.0_1.0_0.0_0.0.csv"))

# Transform the enforcer actions from 2-d to 1-d.
decider_1 = decider_0 %>%
  mutate(decider_reward=decider_reward_0-decider_reward_1,
         enforcer_action=enforcer_action_0-enforcer_action_1)

# Filter the relevant model predictions for the (standard) decider model.
decider_2 = decider_1 %>%
  filter(rationality==0.1, decider_reward %in% c(1, 2, 3),
         ToM %in% c(0.0, 1.0), method=="confidence", cooperation==10,
         enforcer_action %in% c(-1, 0, 1, 2, 3, 4)) %>%
  group_by(ToM, enforcer_action) %>%
  summarize(decider_action=mean(decider_action_1))

# Plot the model predictions for the (standard) enforcer model.
plot_1 = decider_2 %>%
  mutate(ToM=factor(ToM)) %>%
  ggplot(aes(x=enforcer_action, y=decider_action, group=ToM, color=ToM)) +
  geom_line(size=2, alpha=0.75) +
  theme_classic() +
  theme(aspect.ratio=1.0) +
  xlab("Objects Stacked") +
  ylab("Swap Probability") +
  scale_color_manual(name="Decider Type:",
                     limits=c("0", "1"),
                     labels=c("No ToM", "ToM"),
                     values=c(color_palette[3], color_palette[2]))
plot_1
```
